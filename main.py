#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡èŠå¤©è®°å½•åˆ†æç³»ç»Ÿ - ä¸»ç¨‹åº

åŠŸèƒ½ï¼š
1. æ•°æ®é¢„å¤„ç†
2. è¯é¢˜åˆ†å‰²
3. QAé—®ç­”å¯¹è¯†åˆ«
4. é‡è¦åº¦è¯„ä¼°
5. ç¬”è®°ç”Ÿæˆ

ä½œè€…: AI Assistant
åˆ›å»ºæ—¶é—´: 2026-01-13
"""

import sys
import json
import yaml
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# å¯¼å…¥æ¨¡å—
from modules.preprocessor import DataCleaner, clean_raw_data
from modules.llm_client import get_llm_client
from modules.topic_segment import TopicSegmenter, segment_topics
from modules.qa_detector import QADetector, detect_qa_pairs
from modules.importance_scorer import ImportanceScorer, calculate_importance
from modules.note_generator import NoteGenerator, generate_notes
from generate_html_report import generate_report


class ChatAnalysisSystem:
    """èŠå¤©è®°å½•åˆ†æç³»ç»Ÿ"""
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        åˆå§‹åŒ–ç³»ç»Ÿ
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self.config_path = config_path
        
        # åˆå§‹åŒ–å„æ¨¡å—
        self.cleaner = DataCleaner(config_path)
        self.segmenter = TopicSegmenter(config_path)
        self.qa_detector = QADetector(config_path)
        self.scorer = ImportanceScorer(config_path)
        self.generator = NoteGenerator(config_path)
        
        # æ•°æ®å­˜å‚¨
        self.raw_messages = []
        self.cleaned_messages = []
        self.topics = []
        self.qa_pairs = []
        self.scores = []
        self.notes = []
    
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def run_full_pipeline(self, input_file: str, use_llm: bool = True,
                          save_intermediate: bool = True) -> Dict:
        """
        è¿è¡Œå®Œæ•´åˆ†ææµç¨‹
        
        Args:
            input_file: è¾“å…¥çš„JSONLæ–‡ä»¶è·¯å¾„
            use_llm: æ˜¯å¦ä½¿ç”¨LLM
            save_intermediate: æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
            
        Returns:
            åˆ†æç»“æœæ‘˜è¦
        """
        print("=" * 60)
        print("ğŸš€ å¾®ä¿¡èŠå¤©è®°å½•åˆ†æç³»ç»Ÿ")
        print("=" * 60)
        print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
        print(f"ä½¿ç”¨LLM: {'æ˜¯' if use_llm else 'å¦'}")
        print("=" * 60)
        
        start_time = datetime.now()
        run_limits = self.config.get('run_limits', {})
        max_topics = run_limits.get('max_topics')
        max_notes = run_limits.get('max_notes')
        progress_every = run_limits.get('progress_every', 5)
        
        try:
            # 1. æ•°æ®é¢„å¤„ç†
            print("\nğŸ“‹ æ­¥éª¤1: æ•°æ®é¢„å¤„ç†...")
            step_start = datetime.now()
            self.cleaned_messages = self.cleaner.clean_file(input_file)
            print(f"âœ… æ¸…æ´—å®Œæˆ: {len(self.cleaned_messages)} æ¡æ¶ˆæ¯")
            print(f"   â±ï¸ è€—æ—¶: {(datetime.now() - step_start).total_seconds():.1f}s")
            
            if save_intermediate:
                self._save_json('cleaned_messages.json', self.cleaned_messages)
            
            # 2. è¯é¢˜åˆ†å‰²
            print("\nğŸ“Š æ­¥éª¤2: è¯é¢˜åˆ†å‰²...")
            step_start = datetime.now()
            light_messages = self._build_light_messages(self.cleaned_messages)
            intent_results = []
            if use_llm:
                llm = get_llm_client()
                intent_results = llm.analyze_conversation_intent(
                    light_messages,
                    window_size=self.segmenter.batch_size
                )
            if intent_results:
                self.topics = self.segmenter.segment_with_intents(light_messages, intent_results)
            else:
                self.topics = self.segmenter.segment(light_messages, use_llm=use_llm)
            if max_topics and len(self.topics) > max_topics:
                print(f"   âš ï¸ è¯é¢˜è¿‡å¤š({len(self.topics)})ï¼ŒæŒ‰é…ç½®åªä¿ç•™å‰ {max_topics} ä¸ª")
                self.topics = self.topics[:max_topics]
            print(f"âœ… åˆ†å‰²å®Œæˆ: {len(self.topics)} ä¸ªè¯é¢˜")
            print(f"   â±ï¸ è€—æ—¶: {(datetime.now() - step_start).total_seconds():.1f}s")
            
            if save_intermediate:
                self._save_json('topics.json', self.topics) 
            
            # 3. QAé—®ç­”å¯¹è¯†åˆ«
            print("\nâ“ æ­¥éª¤3: QAé—®ç­”å¯¹è¯†åˆ«...")
            step_start = datetime.now()
            light_messages = self._build_light_messages(self.cleaned_messages)
            if use_llm and intent_results:
                self.qa_pairs = self.qa_detector.detect_with_intents(light_messages, intent_results)
            else:
                self.qa_pairs = self.qa_detector.detect(light_messages, use_llm=use_llm)
            print(f"âœ… æ£€æµ‹å®Œæˆ: {len(self.qa_pairs)} ä¸ªQAå¯¹")
            print(f"   â±ï¸ è€—æ—¶: {(datetime.now() - step_start).total_seconds():.1f}s")
            
            if save_intermediate:
                self._save_json('qa_pairs.json', self.qa_pairs)
            
            # 4. é‡è¦åº¦è¯„ä¼°
            print("\nâ­ æ­¥éª¤4: é‡è¦åº¦è¯„ä¼°...")
            step_start = datetime.now()
            threshold_value = 4.0
            self.scorer.threshold = threshold_value
            qa_pairs_list = [self._filter_qa_pairs_for_topic(topic, self.qa_pairs) for topic in self.topics]
            self.scores = self.scorer.batch_score(
                self.topics, 
                qa_pairs_list,
                use_llm=use_llm
            )
            print(f"   â±ï¸ è€—æ—¶: {(datetime.now() - step_start).total_seconds():.1f}s")
            
            # ç»Ÿè®¡
            stats = self.scorer.get_statistics(self.scores)
            print(f"âœ… è¯„ä¼°å®Œæˆ:")
            print(f"   - æ€»è¯é¢˜æ•°: {stats.get('total_topics', len(self.scores))}")
            print(f"   - é€šè¿‡é˜ˆå€¼: {stats.get('passed_topics', 0)} ({stats.get('pass_rate', 0)}%)")
            print(f"   - å¹³å‡åˆ†: {stats.get('average_score', 0)}")
            
            if save_intermediate:
                self._save_json('scores.json', self.scores)
            
            # 5. ç¬”è®°ç”Ÿæˆ
            print("\nğŸ“ æ­¥éª¤5: ç¬”è®°ç”Ÿæˆ...")
            step_start = datetime.now()
            threshold = getattr(self.scorer, "threshold", 4.0)
            passed_topics = self.scorer.get_topics_by_threshold(self.scores, threshold)
            print(f"âœ… é€šè¿‡é˜ˆå€¼(â‰¥{threshold})çš„è¯é¢˜: {len(passed_topics)} ä¸ª")
            
            if passed_topics:
                # ä¸ºé€šè¿‡é˜ˆå€¼çš„è¯é¢˜ç”Ÿæˆç¬”è®°
                note_results = []
                total_topics = len(self.topics)
                for i, topic in enumerate(self.topics):
                    score = self.scores[i]
                    if score.get('pass_threshold', False):
                        result = self.generator.generate(topic, score, self.qa_pairs, use_llm=use_llm)
                        note_results.append(result)
                        if max_notes and len(note_results) >= max_notes:
                            print(f"   âš ï¸ å·²è¾¾åˆ°ç¬”è®°æ•°é‡ä¸Šé™({max_notes})ï¼Œæå‰ç»“æŸç”Ÿæˆ")
                            break
                        if len(note_results) % progress_every == 0 or len(note_results) == 1:
                            print(f"   âœ… è¿›åº¦ {len(note_results)}/{len(passed_topics)} - {result['filename']}")
                
                self.notes = note_results
            print(f"   â±ï¸ è€—æ—¶: {(datetime.now() - step_start).total_seconds():.1f}s")
            
            # è®¡ç®—è€—æ—¶
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # ç”Ÿæˆç»“æœæ‘˜è¦
            result = {
                'status': 'success',
                'duration': round(duration, 2),
                'input_file': input_file,
                'total_messages': len(self.cleaned_messages),
                'total_topics': len(self.topics),
                'total_qa_pairs': len(self.qa_pairs),
                'topics_above_threshold': len(passed_topics),
                'notes_generated': len(self.notes),
                'statistics': {
                    'message_stats': self.cleaner.get_statistics(self.cleaned_messages),
                    'topic_stats': stats,
                    'qa_stats': self.qa_detector.get_qa_statistics(self.qa_pairs)
                },
                'output_dir': self.generator.output_dir
            }
            
            print("\n" + "=" * 60)
            print("âœ… åˆ†æå®Œæˆ!")
            print(f"ğŸ“Š æ€»è€—æ—¶: {duration:.2f} ç§’")
            print(f"ğŸ’¬ æ¶ˆæ¯æ•°: {len(self.cleaned_messages)}")
            print(f"ğŸ“‘ è¯é¢˜æ•°: {len(self.topics)}")
            print(f"â“ QAå¯¹æ•°: {len(self.qa_pairs)}")
            print(f"ğŸ“ ç”Ÿæˆçš„ç¬”è®°: {len(self.notes)}")
            print(f"ğŸ“ ç¬”è®°ä½ç½®: {self.generator.output_dir}")
            print("=" * 60)
            
            return result
            
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                'status': 'error',
                'error': str(e)
            }
    
    def _save_json(self, filename: str, data: Any):
        """ä¿å­˜JSONæ–‡ä»¶"""
        output_dir = Path(self.config.get('paths', {}).get('processed_data', './data/processed'))
        output_dir.mkdir(parents=True, exist_ok=True)
        
        filepath = output_dir / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"   ğŸ’¾ ä¿å­˜ä¸­é—´ç»“æœ: {filepath}")
    
    def run_step_by_step(self, input_file: str, use_llm: bool = True):
        """
        åˆ†æ­¥è¿è¡Œï¼ˆå¯äº¤äº’é€‰æ‹©ï¼‰
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            use_llm: æ˜¯å¦ä½¿ç”¨LLM
        """
        print("\nğŸ”§ åˆ†æ­¥è¿è¡Œæ¨¡å¼")
        print("1. æ•°æ®é¢„å¤„ç†")
        print("2. è¯é¢˜åˆ†å‰²")
        print("3. QAæ£€æµ‹")
        print("4. é‡è¦åº¦è¯„ä¼°")
        print("5. ç¬”è®°ç”Ÿæˆ")
        print("0. å…¨éƒ¨è¿è¡Œ")
        
        step = input("\nè¯·é€‰æ‹©æ­¥éª¤: ").strip()
        
        if step == '0' or step == '':
            self.run_full_pipeline(input_file, use_llm)
        elif step in ['1', '2', '3', '4', '5']:
            self._run_single_step(int(step), input_file, use_llm)
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
    
    def _run_single_step(self, step: int, input_file: str, use_llm: bool):
        """è¿è¡Œå•ä¸ªæ­¥éª¤"""
        steps = {
            1: ("æ•°æ®é¢„å¤„ç†", lambda: self._run_step1(input_file)),
            2: ("è¯é¢˜åˆ†å‰²", lambda: self._run_step2()),
            3: ("QAæ£€æµ‹", lambda: self._run_step3()),
            4: ("é‡è¦åº¦è¯„ä¼°", lambda: self._run_step4()),
            5: ("ç¬”è®°ç”Ÿæˆ", lambda: self._run_step5())
        }
        
        if step not in steps:
            print("âŒ æ— æ•ˆæ­¥éª¤")
            return
        
        name, func = steps[step]
        print(f"\nè¿è¡Œæ­¥éª¤ {step}: {name}")
        func()
    
    def _run_step1(self, input_file: str):
        """è¿è¡Œæ­¥éª¤1: æ•°æ®é¢„å¤„ç†"""
        self.cleaned_messages = self.cleaner.clean_file(input_file)
        print(f"âœ… å®Œæˆ: {len(self.cleaned_messages)} æ¡æ¶ˆæ¯")
    
    def _run_step2(self):
        """è¿è¡Œæ­¥éª¤2: è¯é¢˜åˆ†å‰²"""
        if not self.cleaned_messages:
            print("âŒ è¯·å…ˆè¿è¡Œæ­¥éª¤1")
            return
        light_messages = self._build_light_messages(self.cleaned_messages)
        llm = get_llm_client()
        intent_results = llm.analyze_conversation_intent(
            light_messages,
            window_size=self.segmenter.batch_size
        )
        if intent_results:
            self.topics = self.segmenter.segment_with_intents(light_messages, intent_results)
        else:
            self.topics = self.segmenter.segment(light_messages)
        print(f"âœ… å®Œæˆ: {len(self.topics)} ä¸ªè¯é¢˜")
    
    def _run_step3(self):
        """è¿è¡Œæ­¥éª¤3: QAæ£€æµ‹"""
        if not self.cleaned_messages:
            print("âŒ è¯·å…ˆè¿è¡Œæ­¥éª¤1")
            return
        light_messages = self._build_light_messages(self.cleaned_messages)
        llm = get_llm_client()
        intent_results = llm.analyze_conversation_intent(
            light_messages,
            window_size=self.qa_detector.batch_size
        )
        if intent_results:
            self.qa_pairs = self.qa_detector.detect_with_intents(light_messages, intent_results)
        else:
            self.qa_pairs = self.qa_detector.detect(light_messages)
        print(f"âœ… å®Œæˆ: {len(self.qa_pairs)} ä¸ªQAå¯¹")

    def _build_light_messages(self, messages: List[Dict]) -> List[Dict]:
        """
        æ„å»ºè½»é‡æ¶ˆæ¯ç»“æ„ï¼Œä»…ä¿ç•™è¯´è¯äººå’Œå†…å®¹ï¼ˆå‡å°‘LLMè´Ÿè½½ï¼‰
        """
        light = []
        for msg in messages:
            light.append({
                "sender_name": msg.get("sender_name", ""),
                "content": msg.get("content", "")
            })
        return light
    
    def _run_step4(self):
        """è¿è¡Œæ­¥éª¤4: é‡è¦åº¦è¯„ä¼°"""
        if not self.topics:
            print("âŒ è¯·å…ˆè¿è¡Œæ­¥éª¤2")
            return
        qa_pairs_list = [self._filter_qa_pairs_for_topic(topic, self.qa_pairs) for topic in self.topics]
        self.scores = self.scorer.batch_score(self.topics, qa_pairs_list)
        print(f"âœ… å®Œæˆ: {len(self.scores)} ä¸ªè¯é¢˜è¯„åˆ†")
    
    def _run_step5(self):
        """è¿è¡Œæ­¥éª¤5: ç¬”è®°ç”Ÿæˆ"""
        if not self.scores:
            print("âŒ è¯·å…ˆè¿è¡Œæ­¥éª¤4")
            return
        self.notes = self.generator.batch_generate(self.topics, self.scores)
        print(f"âœ… å®Œæˆ: {len(self.notes)} ä¸ªç¬”è®°")

    def run_full_llm(self, input_file: str, save_intermediate: bool = True) -> Dict:
        """
        å…¨é‡è®°å½•ä¸€æ¬¡æ€§é€å…¥LLMæ€»ç»“ï¼ˆé«˜è´¨é‡æ¨¡å‹ï¼‰
        """
        print("=" * 60)
        print("ğŸš€ å…¨é‡LLMæ±‡æ€»æ¨¡å¼")
        print("=" * 60)
        print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
        print("=" * 60)

        start_time = datetime.now()
        try:
            print("\nğŸ“‹ æ­¥éª¤1: æ•°æ®é¢„å¤„ç†...")
            step_start = datetime.now()
            self.cleaned_messages = self.cleaner.clean_file(input_file)
            print(f"âœ… æ¸…æ´—å®Œæˆ: {len(self.cleaned_messages)} æ¡æ¶ˆæ¯")
            print(f"   â±ï¸ è€—æ—¶: {(datetime.now() - step_start).total_seconds():.1f}s")

            if save_intermediate:
                self._save_json('cleaned_messages.json', self.cleaned_messages)

            print("\nğŸ§  æ­¥éª¤2: å…¨é‡LLMæ±‡æ€»...")
            step_start = datetime.now()
            llm = get_llm_client()
            model = self.config.get('llm', {}).get('full_llm_model', 'doubao-seed-1-8-251228')
            stats = self._compute_activity_stats(self.cleaned_messages)
            
            prompt = self._build_full_llm_prompt(self.cleaned_messages)
            response = llm.chat([
                {"role": "system", "content": "ä½ æ˜¯ç¾¤èŠå†…å®¹åˆ†æä¸“å®¶ï¼Œè¯·è¾“å‡ºç»“æ„åŒ–æ€»ç»“ã€‚"},
                {"role": "user", "content": prompt}
            ], temperature=0.4, model=model)

            report_date = datetime.now().strftime('%Y-%m-%d')
            stem = Path(input_file).stem
            summary_dir = Path(self.config.get("note_generator", {}).get("output_dir", "./notes"))
            summary_path = summary_dir / f"summary_{stem}_{report_date}.md"
            summary_path.write_text(response, encoding="utf-8")

            html_path = Path(f"{stem}_{report_date}_output.html")
            html_content = response
            if not self._looks_like_html(response):
                report_title = self._build_report_title(input_file)
                html_content = self._convert_to_html_with_llm(response, model, report_title, stats)
            else:
                html_content = self._inject_stats_html(html_content, stats)
            html_path.write_text(html_content, encoding="utf-8")

            parsed_summary = self._parse_markdown_summary(response)
            summary_json_path = summary_dir / f"summary_{stem}_{report_date}.json"
            summary_json_path.write_text(
                json.dumps(parsed_summary, ensure_ascii=False, indent=2),
                encoding="utf-8"
            )

            print(f"âœ… æ±‡æ€»å®Œæˆ: {summary_path} / {html_path}")
            print(f"   â±ï¸ è€—æ—¶: {(datetime.now() - step_start).total_seconds():.1f}s")

            duration = (datetime.now() - start_time).total_seconds()
            return {
                'status': 'success',
                'duration': round(duration, 2),
                'input_file': input_file,
                'total_messages': len(self.cleaned_messages),
                'summary_path': str(summary_path),
                'summary_json_path': str(summary_json_path),
                'html_path': str(html_path)
            }
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return {'status': 'error', 'error': str(e)}

    def _looks_like_html(self, text: str) -> bool:
        lowered = text.lstrip().lower()
        return lowered.startswith("<!doctype html") or lowered.startswith("<html")

    def _convert_to_html_with_llm(self, markdown_text: str, model: str,
                                  report_title: str, stats: Dict[str, Any]) -> str:
        """
        ä½¿ç”¨LLMå°†Markdownæ€»ç»“è½¬æ¢ä¸ºHTML
        """
        llm = get_llm_client()
        stats_block = self._build_stats_prompt_block(stats)
        reference_css = self._get_reference_css()
        prompt = f"""
è¯·å°†ä»¥ä¸‹Markdownå†…å®¹è½¬æ¢ä¸ºå®Œæ•´HTMLæ–‡æ¡£ï¼ˆåŒ…å« <html><head><body>ï¼‰ã€‚
è¦æ±‚ï¼šä¿ç•™å±‚çº§ç»“æ„ä¸åˆ—è¡¨ï¼Œä¸è¦é—æ¼å†…å®¹ã€‚
é£æ ¼è¦æ±‚ï¼šæ–°å¸ƒé²æ‰˜ä¸»ä¹‰ï¼ˆNeubrutalismï¼‰ï¼Œé«˜å¯¹æ¯”ã€åšè¾¹æ¡†ã€å—çŠ¶å¡ç‰‡ã€å¼ºçƒˆè‰²å—ï¼›ä¸¥æ ¼å‚è€ƒä¸‹æ–¹CSSé£æ ¼ä¸é…è‰²ã€‚
æ ‡é¢˜è¦æ±‚ï¼šä¸»æ ‡é¢˜å›ºå®šä¸ºâ€œ{report_title}â€ï¼Œå‰¯æ ‡é¢˜å¯åŒ…å«ç”Ÿæˆæ—¶é—´æˆ–æ•°æ®é‡ã€‚
å¸ƒå±€è¦æ±‚ï¼šé¡¶éƒ¨æ ‡é¢˜åŒº + å…³é”®æŒ‡æ ‡æ¨ªå‘å¡ç‰‡ + æ•°æ®åˆ†ææ¿å— + ä¸»é¢˜å¡ç‰‡ç½‘æ ¼ + æ·±åº¦æ€»ç»“åŒºï¼›ä½¿ç”¨æ …æ ¼å¸ƒå±€ï¼Œç•™ç™½å……è¶³ï¼Œé¿å…å‘†æ¿ã€‚
å†…å®¹è¦æ±‚ï¼šæ ‡é¢˜æ¸…æ™°ï¼Œåˆ—è¡¨é¡¹å¯è¯»æ€§é«˜ï¼Œå¡ç‰‡å±‚çº§åˆ†æ˜ã€‚
è¯·æ–°å¢â€œæ•°æ®åˆ†æâ€æ¿å—ï¼ŒåŒ…å«ï¼šæ´»è·ƒäººæ•°ã€æ´»è·ƒæ—¶æ®µç»Ÿè®¡å›¾ï¼ˆæŒ‰å°æ—¶æŸ±çŠ¶å›¾å³å¯ï¼‰ã€æ‘¸é±¼æ¦œï¼ˆåºŸè¯æ¦œï¼Œå­—æ•°å°‘äº6ï¼‰ã€ç¡¬æ ¸æ¦œï¼ˆéåºŸè¯ï¼‰ã€‚
{stats_block}

å‚è€ƒCSSï¼ˆè¯·ä¿æŒé£æ ¼ä¸€è‡´ï¼Œå¯æŒ‰éœ€ç®€åŒ–ä½†ä¸è¦åç¦»é…è‰²/å­—é‡/è¾¹æ¡†é£æ ¼ï¼‰ï¼š
<style>
{reference_css}
</style>

Markdownå†…å®¹ï¼š
{markdown_text}
"""
        return llm.chat([
            {"role": "system", "content": "ä½ æ˜¯HTMLæ ¼å¼åŒ–ä¸“å®¶ï¼Œåªè¾“å‡ºHTMLæ–‡æœ¬ã€‚"},
            {"role": "user", "content": prompt}
        ], temperature=0.2, model=model)

    def _build_stats_prompt_block(self, stats: Dict[str, Any]) -> str:
        if not stats:
            return ""
        active_users = stats.get("active_users", 0)
        fish_rank = stats.get("fish_rank", [])
        hardcore_rank = stats.get("hardcore_rank", [])
        hourly_counts = stats.get("hourly_counts", {})
        lines = [
            "ç»Ÿè®¡æ•°æ®ï¼ˆè¯·åœ¨HTMLä¸­ä½¿ç”¨è¿™äº›æ•°æ®ï¼‰ï¼š",
            f"- æ´»è·ƒäººæ•°: {active_users}",
            "- æ´»è·ƒæ—¶æ®µï¼ˆå°æ—¶: æ¡æ•°ï¼‰: " + ", ".join(
                [f"{hour}: {count}" for hour, count in sorted(hourly_counts.items())]
            ),
            "- æ‘¸é±¼æ¦œï¼ˆç”¨æˆ·: æ¬¡æ•°ï¼‰: " + ", ".join(
                [f"{item['name']}: {item['count']}" for item in fish_rank]
            ),
            "- ç¡¬æ ¸æ¦œï¼ˆç”¨æˆ·: æ¬¡æ•°ï¼‰: " + ", ".join(
                [f"{item['name']}: {item['count']}" for item in hardcore_rank]
            ),
        ]
        return "\n".join(lines)

    def _get_reference_css(self) -> str:
        return """
* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

body {
    font-family: 'Arial Black', Arial, sans-serif;
    background-color: #f0f0f0;
    padding: 20px;
    line-height: 1.6;
}

/* æ ‡é¢˜åŒºæ ·å¼ */
.header {
    background-color: #ff6b6b;
    border: 4px solid #000;
    padding: 25px;
    margin-bottom: 25px;
    border-radius: 0;
}

.header h1 {
    font-size: 2.5rem;
    color: #000;
    margin-bottom: 12px;
}

.header p {
    font-size: 1.2rem;
    color: #333;
    font-weight: bold;
}

/* å…³é”®æŒ‡æ ‡åŒºæ ·å¼ */
.key-metrics {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
    gap: 20px;
    margin-bottom: 35px;
}

.metric-card {
    background-color: #4ecdc4;
    border: 4px solid #000;
    padding: 20px;
    text-align: center;
    transition: transform 0.2s;
}

.metric-card:hover {
    transform: translateY(-5px);
}

.metric-card:nth-child(2) {
    background-color: #ffe66d;
}

.metric-card:nth-child(3) {
    background-color: #ffd166;
}

.metric-card:nth-child(4) {
    background-color: #1a535c;
    color: #fff;
}

.metric-card h3 {
    font-size: 1.4rem;
    margin-bottom: 10px;
}

.metric-card .value {
    font-size: 2.2rem;
    font-weight: 900;
}

/* æ•°æ®åˆ†æåŒºæ ·å¼ */
.data-analysis {
    background-color: #fff;
    border: 4px solid #000;
    padding: 25px;
    margin-bottom: 35px;
}

.data-analysis h2 {
    font-size: 2.2rem;
    margin-bottom: 20px;
    border-bottom: 4px solid #000;
    padding-bottom: 10px;
}

.section {
    margin-bottom: 30px;
}

.section h3 {
    font-size: 1.5rem;
    margin-bottom: 15px;
    background-color: #ffe66d;
    display: inline-block;
    padding: 6px 12px;
    border: 2px solid #000;
}

/* æ´»è·ƒæ—¶æ®µæŸ±çŠ¶å›¾ */
.activity-chart {
    display: flex;
    align-items: flex-end;
    height: 220px;
    gap: 6px;
    padding: 15px;
    background-color: #f0f0f0;
    border: 3px solid #000;
}

.chart-bar {
    flex: 1;
    background-color: #4ecdc4;
    border: 2px solid #000;
    position: relative;
    transition: background-color 0.2s;
}

.chart-bar:hover {
    background-color: #ff6b6b;
}

.chart-bar::after {
    content: attr(data-hour);
    position: absolute;
    bottom: -22px;
    left: 50%;
    transform: translateX(-50%);
    font-size: 0.9rem;
    font-weight: bold;
}

.chart-bar .count {
    position: absolute;
    top: -22px;
    left: 50%;
    transform: translateX(-50%);
    font-size: 0.8rem;
    font-weight: 900;
}

/* æ’è¡Œæ¦œæ ·å¼ */
.rank-list {
    list-style: none;
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
    gap: 12px;
}

.rank-list li {
    background-color: #f7fff7;
    border: 2px solid #000;
    padding: 10px 15px;
    display: flex;
    justify-content: space-between;
    transition: background-color 0.2s;
}

.rank-list li:hover {
    background-color: #e6f9e6;
}

.rank-list li:nth-child(odd) {
    background-color: #edf7f6;
}

.rank-list .user {
    font-weight: bold;
}

.rank-list .count {
    background-color: #ff6b6b;
    padding: 2px 8px;
    border-radius: 4px;
    font-weight: 900;
}

/* ä¸»é¢˜å¡ç‰‡åŒºæ ·å¼ */
.topic-cards {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(380px, 1fr));
    gap: 25px;
    margin-bottom: 35px;
}

.topic-card {
    background-color: #fff;
    border: 4px solid #000;
    padding: 22px;
    transition: transform 0.2s;
}

.topic-card:hover {
    transform: translateY(-5px);
}

.topic-card h2 {
    font-size: 1.8rem;
    margin-bottom: 15px;
    background-color: #4ecdc4;
    display: inline-block;
    padding: 6px 12px;
    border: 2px solid #000;
}

.topic-card h3 {
    font-size: 1.3rem;
    margin: 12px 0;
    color: #ff6b6b;
    border-left: 6px solid #ff6b6b;
    padding-left: 10px;
}

.topic-card ul {
    margin-left: 25px;
    margin-bottom: 18px;
}

.topic-card ul li {
    margin-bottom: 10px;
    font-weight: 500;
}

.topic-card a {
    color: #1a535c;
    text-decoration: underline;
    font-weight: bold;
}

.topic-card a:hover {
    color: #ff6b6b;
}

/* æ·±åº¦æ€»ç»“åŒºæ ·å¼ */
.summary {
    background-color: #ffe66d;
    border: 4px solid #000;
    padding: 25px;
}

.summary h2 {
    font-size: 2.2rem;
    margin-bottom: 18px;
    border-bottom: 4px solid #000;
    padding-bottom: 10px;
}

.summary p {
    font-size: 1.15rem;
    margin-bottom: 12px;
    font-weight: 500;
}
"""

    def _parse_markdown_summary(self, markdown_text: str) -> Dict[str, Any]:
        section_map = {
            "æ€»è§ˆ": "overview",
            "å…³é”®ä¸»é¢˜": "key_topics",
            "é‡è¦è§‚ç‚¹ä¸å…±è¯†": "viewpoints",
            "äº‰è®®ä¸åˆ†æ­§": "disputes",
            "é—®ç­”ç²¾é€‰": "qa_pairs",
            "å¯æ‰§è¡Œå»ºè®®": "suggestions",
            "å‚è€ƒèµ„æº": "resources",
        }
        result = {
            "overview": "",
            "key_topics": [],
            "viewpoints": [],
            "disputes": [],
            "qa_pairs": [],
            "suggestions": [],
            "resources": [],
        }
        current_key = ""
        lines = markdown_text.splitlines()
        for raw_line in lines:
            line = raw_line.strip()
            if not line:
                continue
            header = line.lstrip("#").strip()
            matched_section = False
            for label, key in section_map.items():
                if label in header:
                    current_key = key
                    matched_section = True
                    trailing = header.split(label, 1)[-1].strip(" ï¼š:ã€.-")
                    if trailing:
                        if key == "overview":
                            result["overview"] = trailing
                        elif key == "qa_pairs":
                            result["qa_pairs"].append({"q": trailing, "a": ""})
                        else:
                            result[key].append(trailing)
                    break
            if matched_section:
                continue

            if current_key == "overview":
                if result["overview"]:
                    result["overview"] += " " + line
                else:
                    result["overview"] = line
            elif current_key in ("key_topics", "viewpoints", "disputes", "suggestions", "resources"):
                item = re.sub(r"^(\d+[\.\)ã€]|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€\.]|[\-\â€¢])\s*", "", line).strip()
                if item:
                    result[current_key].append(item)
            elif current_key == "qa_pairs":
                qa_match = re.match(r"^(Q|é—®|A|ç­”)\s*[:ï¼š]\s*(.*)$", line)
                if qa_match:
                    tag = qa_match.group(1)
                    text = qa_match.group(2).strip()
                    if tag in ("Q", "é—®"):
                        result["qa_pairs"].append({"q": text, "a": ""})
                    else:
                        if result["qa_pairs"]:
                            result["qa_pairs"][-1]["a"] = text
                        else:
                            result["qa_pairs"].append({"q": "", "a": text})
                else:
                    qa_inline = re.match(r"^Q\s*[ï¼š:]\s*(.+?)\s*A\s*[ï¼š:]\s*(.+)$", line)
                    if qa_inline:
                        result["qa_pairs"].append({"q": qa_inline.group(1).strip(), "a": qa_inline.group(2).strip()})
                    else:
                        result["qa_pairs"].append({"q": line, "a": ""})
        return result

    def _full_llm_styles(self) -> str:
        return """
<style>
* { margin:0; padding:0; box-sizing:border-box; }
body { font-family:'Arial Black', Arial, sans-serif; background:#f0f0f0; padding:20px; line-height:1.6; }
.header { background:#ff6b6b; border:4px solid #000; padding:25px; margin-bottom:25px; }
.header h1 { font-size:2.5rem; color:#000; margin-bottom:12px; }
.header p { font-size:1.2rem; color:#333; font-weight:bold; }
.key-metrics { display:grid; grid-template-columns:repeat(auto-fit, minmax(220px, 1fr)); gap:20px; margin-bottom:35px; }
.metric-card { background:#4ecdc4; border:4px solid #000; padding:20px; text-align:center; }
.metric-card:nth-child(2) { background:#ffe66d; }
.metric-card:nth-child(3) { background:#ffd166; }
.metric-card:nth-child(4) { background:#1a535c; color:#fff; }
.metric-card h3 { font-size:1.4rem; margin-bottom:10px; }
.metric-card .value { font-size:2.2rem; font-weight:900; }
.topic-cards { display:grid; grid-template-columns:repeat(auto-fit, minmax(380px, 1fr)); gap:25px; margin-bottom:35px; }
.topic-card { background:#fff; border:4px solid #000; padding:22px; }
.topic-card h2 { font-size:1.8rem; margin-bottom:15px; background:#4ecdc4; display:inline-block; padding:6px 12px; border:2px solid #000; }
.topic-card ul { margin-left:18px; }
.topic-card li { margin-bottom:6px; }
</style>
"""

    def _compute_activity_stats(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        user_set = set()
        fish_counts: Dict[str, int] = {}
        hardcore_counts: Dict[str, int] = {}
        hourly_counts: Dict[int, int] = {h: 0 for h in range(24)}
        for msg in messages:
            sender = msg.get("sender_name") or "Unknown"
            content = (msg.get("content") or "").strip()
            timestamp = msg.get("timestamp")
            user_set.add(sender)
            if timestamp:
                hour = datetime.fromtimestamp(timestamp).hour
                hourly_counts[hour] = hourly_counts.get(hour, 0) + 1
            if not content:
                continue
            if len(content) < 6:
                fish_counts[sender] = fish_counts.get(sender, 0) + 1
            else:
                hardcore_counts[sender] = hardcore_counts.get(sender, 0) + 1
        fish_rank = sorted(
            [{"name": name, "count": count} for name, count in fish_counts.items()],
            key=lambda item: item["count"],
            reverse=True
        )[:10]
        hardcore_rank = sorted(
            [{"name": name, "count": count} for name, count in hardcore_counts.items()],
            key=lambda item: item["count"],
            reverse=True
        )[:10]
        return {
            "active_users": len(user_set),
            "hourly_counts": hourly_counts,
            "fish_rank": fish_rank,
            "hardcore_rank": hardcore_rank
        }

    def _build_stats_html(self, stats: Dict[str, Any]) -> str:
        if not stats:
            return ""
        hourly_counts = stats.get("hourly_counts", {})
        max_count = max(hourly_counts.values()) if hourly_counts else 0
        bars = []
        for hour in range(24):
            count = hourly_counts.get(hour, 0)
            height = 0 if max_count == 0 else int((count / max_count) * 120)
            bars.append(
                f'<div class="hour-bar"><span>{hour:02d}</span>'
                f'<div class="bar" style="height:{height}px"></div>'
                f'<em>{count}</em></div>'
            )
        fish_items = "".join(
            [f"<li>{item['name']}: {item['count']}</li>" for item in stats.get("fish_rank", [])]
        ) or "<li>æš‚æ— </li>"
        hardcore_items = "".join(
            [f"<li>{item['name']}: {item['count']}</li>" for item in stats.get("hardcore_rank", [])]
        ) or "<li>æš‚æ— </li>"
        return f"""
<section class="section data-analysis">
  <h2>æ•°æ®åˆ†æ</h2>
  <div class="stats-row">
    <div class="stat-card">
      <h3>æ´»è·ƒäººæ•°</h3>
      <p class="stat-number">{stats.get('active_users', 0)}</p>
    </div>
  </div>
  <div class="hourly-chart">
    <h3>æ´»è·ƒæ—¶æ®µï¼ˆæŒ‰å°æ—¶ï¼‰</h3>
    <div class="hour-bars">
      {''.join(bars)}
    </div>
  </div>
  <div class="rankings">
    <div class="rank-card">
      <h3>æ‘¸é±¼æ¦œï¼ˆåºŸè¯æ¦œï¼‰</h3>
      <ol>{fish_items}</ol>
    </div>
    <div class="rank-card">
      <h3>ç¡¬æ ¸æ¦œ</h3>
      <ol>{hardcore_items}</ol>
    </div>
  </div>
</section>
"""

    def _stats_inline_styles(self) -> str:
        return """
<style>
.data-analysis .stats-row { display:flex; gap:20px; margin-bottom:24px; flex-wrap:wrap; }
.data-analysis .stat-card { background:#161625; border:3px solid #ffffff; padding:16px 20px; min-width:180px; }
.data-analysis .stat-number { font-size:32px; margin:8px 0 0; }
.hourly-chart { margin:24px 0; }
.hour-bars { display:grid; grid-template-columns:repeat(12, minmax(0, 1fr)); gap:12px; align-items:end; }
.hour-bar { display:flex; flex-direction:column; align-items:center; gap:6px; font-size:12px; }
.hour-bar .bar { width:100%; background:#00f0ff; border:2px solid #ffffff; }
.rankings { display:grid; grid-template-columns:repeat(auto-fit, minmax(220px, 1fr)); gap:20px; }
.rank-card { background:#161625; border:3px solid #ffffff; padding:16px 20px; }
.rank-card ol { margin:10px 0 0 20px; }
</style>
"""

    def _build_report_title(self, input_file: str) -> str:
        """
        æ ¹æ®è¾“å…¥æ–‡ä»¶åç”ŸæˆæŠ¥å‘Šæ ‡é¢˜
        """
        stem = Path(input_file).stem
        if not stem:
            return "ç¾¤èŠåˆ†æ"
        return f"{stem} ç¾¤èŠåˆ†æ"

    def _inject_stats_html(self, html: str, stats: Dict[str, Any]) -> str:
        stats_html = self._build_stats_html(stats)
        styles = self._stats_inline_styles()
        if not stats_html:
            return html
        lower_html = html.lower()
        head_idx = lower_html.find("</head>")
        if head_idx != -1:
            html = html[:head_idx] + styles + html[head_idx:]
            lower_html = html.lower()
        insert_idx = lower_html.rfind("</body>")
        if insert_idx == -1:
            return html + styles + stats_html
        return html[:insert_idx] + stats_html + html[insert_idx:]
    def _build_full_llm_prompt(self, messages: List[Dict]) -> str:
        """
        æ„é€ å…¨é‡LLMæ±‡æ€»æç¤ºè¯
        """
        lines = []
        for msg in messages:
            sender = msg.get("sender_name", "Unknown")
            content = msg.get("content", "")
            if not content:
                continue
            lines.append(f"{sender}: {content}")

        merged = "\n".join(lines)
        prompt = f"""
ä½ æ˜¯ç¾¤èŠå†…å®¹åˆ†æä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹å®Œæ•´èŠå¤©è®°å½•ç”Ÿæˆç»“æ„åŒ–æ€»ç»“æŠ¥å‘Šã€‚æ³¨æ„ï¼šèŠå¤©è®°å½•ä¸­çš„é“¾æ¥å·²è¢«æ ‡å‡†åŒ–ä¸ºã€æè¿°ã€‘<URL>æ ¼å¼ã€‚

è¾“å‡ºè¦æ±‚ï¼ˆMarkdownï¼‰ï¼š
1. æ€»è§ˆï¼ˆ1æ®µï¼Œ50~120å­—ï¼‰
2. å…³é”®ä¸»é¢˜ï¼ˆ3~8æ¡ï¼Œæ¯æ¡ä¸€å¥ï¼‰
3. é‡è¦è§‚ç‚¹ä¸å…±è¯†ï¼ˆ3~8æ¡ï¼‰
4. äº‰è®®ä¸åˆ†æ­§ï¼ˆå¦‚æ— åˆ™å†™â€œæ— æ˜æ˜¾äº‰è®®â€ï¼‰
5. é—®ç­”ç²¾é€‰ï¼ˆ5æ¡ï¼ŒQ/A å½¢å¼ï¼Œå¿…é¡»æ¥è‡ªåŸæ–‡è¯­å¥ï¼‰
6. å¯æ‰§è¡Œå»ºè®®ï¼ˆ3~6æ¡ï¼Œå…·ä½“å¯è½åœ°ï¼‰
7. å‚è€ƒèµ„æºï¼ˆä»…åˆ—å‡ºæ–‡ä¸­å‡ºç°çš„é“¾æ¥ï¼Œä¿ç•™åŸURLï¼‰

è§„åˆ™ï¼š
- ä¸è¦ç¼–é€ æœªå‡ºç°çš„ä¿¡æ¯
- åˆå¹¶ç›¸è¿‘è§‚ç‚¹ï¼Œé¿å…é‡å¤
- å¦‚æŸéƒ¨åˆ†ä¿¡æ¯ä¸è¶³ï¼Œå†™â€œæš‚æ— â€

èŠå¤©è®°å½•ï¼š
{merged}
"""
        return prompt

    def _filter_qa_pairs_for_topic(self, topic: Dict, qa_pairs: List[Dict]) -> List[Dict]:
        """
        æ ¹æ®è¯é¢˜è¾¹ç•Œè¿‡æ»¤QAå¯¹
        """
        if not qa_pairs:
            return []
        start_idx = topic.get('start_index', 0)
        end_idx = topic.get('end_index', -1)
        filtered = []
        for pair in qa_pairs:
            q_idx = pair.get('question_index')
            a_idx = pair.get('answer_index')
            if q_idx is not None and start_idx <= q_idx <= end_idx:
                filtered.append(pair)
            elif a_idx is not None and start_idx <= a_idx <= end_idx:
                filtered.append(pair)
        return filtered


def get_timestamp_range(target_date: str) -> tuple:
    """
    è·å–ç›®æ ‡æ—¥æœŸçš„æ—¶é—´æˆ³èŒƒå›´
    """
    normalized_date = _normalize_date(target_date)
    try:
        date_obj = datetime.strptime(normalized_date, "%Y-%m-%d")
    except ValueError as exc:
        raise ValueError(
            f"æ—¥æœŸæ ¼å¼é”™è¯¯: {target_date}ï¼Œåº”ä¸º YYYY-MM-DDï¼Œä¾‹å¦‚ 2026-01-20"
        ) from exc
    start_datetime = datetime(
        date_obj.year,
        date_obj.month,
        date_obj.day,
        0, 0, 0
    )
    end_datetime = datetime(
        date_obj.year,
        date_obj.month,
        date_obj.day,
        23, 59, 59
    )
    return int(start_datetime.timestamp()), int(end_datetime.timestamp())


def extract_messages_by_date(
    file_path: str,
    target_date: str,
    message_types: Optional[List[int]] = None
) -> List[Dict[str, Any]]:
    """
    ä»JSONLæ–‡ä»¶ä¸­æå–æŒ‡å®šæ—¥æœŸçš„æ¶ˆæ¯
    """
    start_ts, end_ts = get_timestamp_range(target_date)
    messages = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            line_number = 0
            for line in f:
                line_number += 1
                try:
                    data = json.loads(line.strip())
                    if data.get('_type') == 'message':
                        timestamp = data.get('timestamp', 0)
                        if start_ts <= timestamp <= end_ts:
                            if message_types is None or data.get('type') in message_types:
                                data['_line_number'] = line_number
                                messages.append(data)
                except json.JSONDecodeError:
                    continue
    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {file_path}")
        return []
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return []
    return messages


def save_messages_to_jsonl(
    messages: List[Dict[str, Any]],
    output_file: str
) -> bool:
    """
    å°†æ¶ˆæ¯ä¿å­˜ä¸ºJSONLæ ¼å¼
    """
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            for msg in messages:
                msg_copy = {k: v for k, v in msg.items() if k != '_line_number'}
                f.write(json.dumps(msg_copy, ensure_ascii=False) + '\n')
        return True
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False


def extract_daily_messages(
    input_file: str,
    target_date: str,
    output_file: Optional[str] = None,
    message_types: Optional[List[int]] = None,
    verbose: bool = True
) -> List[Dict[str, Any]]:
    """
    æå–æŒ‡å®šæ—¥æœŸçš„æ¶ˆæ¯
    """
    if verbose:
        print(f"æ­£åœ¨ä» {input_file} ä¸­æå– {target_date} çš„æ¶ˆæ¯...")
    try:
        messages = extract_messages_by_date(input_file, target_date, message_types)
    except ValueError as exc:
        print(f"é”™è¯¯: {exc}")
        return []
    if verbose:
        print(f"æ‰¾åˆ° {len(messages)} æ¡æ¶ˆæ¯")
    if output_file:
        if save_messages_to_jsonl(messages, output_file):
            if verbose:
                print(f"æ¶ˆæ¯å·²ä¿å­˜åˆ° {output_file}")
        else:
            if verbose:
                print("ä¿å­˜å¤±è´¥")
    return messages


def get_file_date_range(file_path: str) -> Dict[str, str]:
    """
    è·å–æ–‡ä»¶ä¸­æ¶ˆæ¯çš„æ—¥æœŸèŒƒå›´
    """
    min_timestamp = None
    max_timestamp = None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line.strip())
                    if data.get('_type') == 'message':
                        timestamp = data.get('timestamp', 0)
                        if min_timestamp is None or timestamp < min_timestamp:
                            min_timestamp = timestamp
                        if max_timestamp is None or timestamp > max_timestamp:
                            max_timestamp = timestamp
                except json.JSONDecodeError:
                    continue
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return {}
    if min_timestamp and max_timestamp:
        return {
            'earliest': datetime.fromtimestamp(min_timestamp).strftime('%Y-%m-%d %H:%M:%S'),
            'latest': datetime.fromtimestamp(max_timestamp).strftime('%Y-%m-%d %H:%M:%S'),
            'earliest_date': datetime.fromtimestamp(min_timestamp).strftime('%Y-%m-%d'),
            'latest_date': datetime.fromtimestamp(max_timestamp).strftime('%Y-%m-%d')
        }
    return {}


def _parse_message_types(raw_value: Optional[str]) -> Optional[List[int]]:
    if not raw_value:
        return None
    return [int(item) for item in raw_value.split(',') if item.strip()]


def _normalize_date(target_date: str) -> str:
    parts = target_date.strip().split("-")
    if len(parts) != 3:
        return target_date
    year, month, day = parts
    if not (year.isdigit() and month.isdigit() and day.isdigit()):
        return target_date
    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"


def _print_extraction_stats(messages: List[Dict[str, Any]]) -> None:
    if not messages:
        print("\næœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ¶ˆæ¯")
        return
    print(f"\næå–ç»Ÿè®¡:")
    print(f"  æ€»æ¶ˆæ¯æ•°: {len(messages)}")
    type_counts: Dict[Any, int] = {}
    for msg in messages:
        msg_type = msg.get('type', 'Unknown')
        type_counts[msg_type] = type_counts.get(msg_type, 0) + 1
    print("  æ¶ˆæ¯ç±»å‹åˆ†å¸ƒ:")
    for msg_type, count in sorted(type_counts.items(), key=lambda item: str(item[0])):
        type_name = {
            0: 'æ–‡æœ¬',
            1: 'å›¾ç‰‡',
            5: 'è¡¨æƒ…',
            80: 'ç³»ç»Ÿ',
            99: 'é“¾æ¥'
        }.get(msg_type, f'ç±»å‹{msg_type}')
        print(f"    {type_name} (Type {msg_type}): {count} æ¡")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å¾®ä¿¡èŠå¤©è®°å½•åˆ†æç³»ç»Ÿ')
    parser.add_argument('input', nargs='?', help='è¾“å…¥çš„JSONLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--config', '-c', default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--no-llm', action='store_true', help='ä¸ä½¿ç”¨LLMï¼ˆä»…è§„åˆ™ï¼‰')
    parser.add_argument('--step', '-s', type=int, choices=[1, 2, 3, 4, 5], help='è¿è¡Œå•ä¸ªæ­¥éª¤')
    parser.add_argument('--full-llm', action='store_true', help='å…¨é‡è®°å½•ä¸€æ¬¡æ€§LLMæ±‡æ€»ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰')
    parser.add_argument('--pipeline', action='store_true', help='ä½¿ç”¨æ ‡å‡†åˆ†æ­¥æµæ°´çº¿ï¼ˆéå…¨é‡æ±‡æ€»ï¼‰')
    parser.add_argument('--info', action='store_true', help='æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯')
    parser.add_argument('--html-report', action='store_true', help='ç”ŸæˆHTMLæŠ¥å‘Š')
    parser.add_argument('--report-date', help='HTMLæŠ¥å‘Šæ—¥æœŸ YYYY-MM-DDï¼ˆé»˜è®¤ä»Šå¤©ï¼‰')
    parser.add_argument('--html-output', help='HTMLæŠ¥å‘Šè¾“å‡ºè·¯å¾„')
    parser.add_argument('--extract-date', help='å…ˆæå–æŒ‡å®šæ—¥æœŸæ¶ˆæ¯ YYYY-MM-DD')
    parser.add_argument('--extract-output', help='æå–åçš„JSONLè¾“å‡ºè·¯å¾„')
    parser.add_argument('--extract-types', help='æå–çš„æ¶ˆæ¯ç±»å‹ï¼Œé€—å·åˆ†éš”ï¼Œå¦‚ 0,1')
    parser.add_argument('--extract-only', action='store_true', help='ä»…æ‰§è¡Œæå–ï¼Œä¸ç»§ç»­åˆ†æ')
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº†--info
    if args.info:
        if args.input:
            date_range = get_file_date_range(args.input)
            if date_range:
                print(f"ğŸ“… æ—¥æœŸèŒƒå›´: {date_range['earliest_date']} - {date_range['latest_date']}")
            cleaner = DataCleaner(args.config)
            messages = cleaner.clean_file(args.input)
            stats = cleaner.get_statistics(messages)
            print(f"ğŸ“Š æ–‡ä»¶ç»Ÿè®¡:")
            print(f"   æ€»æ¶ˆæ¯æ•°: {stats['total_messages']}")
            print(f"   å‚ä¸äººæ•°: {stats['participant_count']}")
            print(f"   é—®é¢˜æ•°: {stats['question_count']}")
            print(f"   æ—¥æœŸèŒƒå›´: {list(stats['date_distribution'].keys())[:5]}...")
        else:
            print("âŒ è¯·æŒ‡å®šè¾“å…¥æ–‡ä»¶")
        return
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not args.input:
        print("âŒ è¯·æŒ‡å®šè¾“å…¥æ–‡ä»¶")
        print("ç”¨æ³•: python main.py <input_file> [--config config.yaml] [--no-llm]")
        print("ç¤ºä¾‹: python main.py chat.jsonl")
        print("     python main.py chat.jsonl --no-llm")
        sys.exit(1)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.input).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        sys.exit(1)

    analysis_input = args.input
    if args.extract_date:
        message_types = _parse_message_types(args.extract_types)
        output_file = args.extract_output
        if not output_file:
            stem = Path(args.input).stem
            output_file = f"{stem}_{args.extract_date}.jsonl"
        extracted_messages = extract_daily_messages(
            input_file=args.input,
            target_date=args.extract_date,
            output_file=output_file,
            message_types=message_types
        )
        _print_extraction_stats(extracted_messages)
        analysis_input = output_file
        if args.extract_only:
            return
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    system = ChatAnalysisSystem(args.config)
    
    # è¿è¡Œ
    if args.step:
        system.run_step_by_step(analysis_input, not args.no_llm)
    elif args.full_llm or not args.pipeline:
        system.run_full_llm(analysis_input)
    else:
        result = system.run_full_pipeline(analysis_input, not args.no_llm)
        if args.html_report and result.get('status') == 'success':
            report_date = args.report_date or args.extract_date or datetime.now().strftime('%Y-%m-%d')
            notes_dir = Path(result.get('output_dir', './notes'))
            if args.html_output:
                output_path = Path(args.html_output)
            else:
                stem = Path(analysis_input).stem
                output_name = f"{stem}_{report_date}_output.html"
                output_path = Path(output_name)
            generate_report(report_date, notes_dir, output_path)
            print(f"âœ… HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")


if __name__ == "__main__":
    main()
